# Summary
<!--- Intution Intution Intution -->
<!--- the WHYs whenever we can -->

<!-- Introduce fun challenges at the end of each section? -->
- [Welcome](./Welcome.md)
- [Installation & Set-up](./Installation%20&%20Set-up.md)
    - [The Libraries](./The$20Libraries)
- [The Math you need]  <!-- You don't need it for MDN but if you want to do xyz then .. -->
- [Deep Learning]
    - [What is Deep Learning]
    - [Types of Learning] <!-- Relevant exapmples -->
    - [Core Concepts of Deep learning]
        - [Datasets]  
        - [Neural network] 
        - [Gradient Descent]
        - [Learning Rates]
        - [Loss Functions] 
        - [Metrics]
        - [Code it up from scratch]

 
- [CNN]
    - [Why not MLPs]
    - [What are CNNs]
    - [Why do CNNs work]
    - [Revolutionary CNNs]
        - [Vanishing Gradient? Introduce Skip Connections]
    - [Limitations]

<!-- I say we leave this to the last -->
- [RNNs] 

- [RL]
- [Trasnformers]
<!--- (Build it using examples from the ground up in text and maybe images ? What problems does it alleviate that CNNs can't overcome) -->
<!--- Compare it to RNNs?-->
<!--  More sections to add here-->

    - [Motivations to Trasnformers]
    - [Transformers vs ex models RNNs etc]
    -[The structure]
        -[Attention mechanism and why]
        -[Self-attention and multi-head attention]
        -[Positional encoding]

        -[Encoder and decoder structures]
        -[Feed-forward layers]
        -[Residual connections and layer normalization]
    - [Popular Transformer Models]



- [Units to take]



