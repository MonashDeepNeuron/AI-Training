#### Notes for later

"Transformers surpassed the previous state-of-the-art approaches based on recurrent neural networks" -> Okay expand on the previous ideas such as RNNs, what were their limitations, the history and all that. Keep it short but comprehensive


"One major advantage of transformers is that transfer learning is very effective" why is that


RNNS are much slower and lose meaning down the line



#### Resources being used

ECE unit 
Deep Learning Foundations and Concepts book

Check this out : <!-- RNN visualizer -->
https://github.com/ma-labo/DeepSeer

https://research.google/blog/transformer-a-novel-neural-network-architecture-for-language-understanding/
----


# Attention Mechanism and Why


The multi-head attention mechanism proposed in this influential paper "Attention is all you need" has initiated the boom in NLP and has influened the popular models we see nowadays such as GPT-3 for the natural language generation. Transformers have been latter applied in other modalities such as in vision, speech and initiated developments in onvolutional neural networks.



### What is Attention 


According to Wikipedia, Attention is a machine learning method that determines the relative importance of each component in a sequence relative to the other components in that sequence.

In the case of Natural Languages, a senetnce is considered a sequence and the words are considered components.


Let us set some ground rules.

Between two components, x_i and x_j, would it make sense if the association between both is a negative number? It is either two components are associated or not associated.










